{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recolección de datos\n",
    "\n",
    "Esta parte la puede detallar mejor Jessica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento y extracción de características\n",
    "\n",
    "Una vez que definimos las columnas, o variables, que utilizaremos en la construcción del dataset recurrimos a reglas del negocio para saber como tratar los valores *en blanco* o nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' libraries'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "''' config notebook ''' \n",
    "%matplotlib inline\n",
    "\n",
    "datos = pd.read_csv('CAToperaciones.csv')\n",
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tenemos ningun dato nulo :) de ello cuidamos en la RECOLECCIÓN DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isnull().values.any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datos['CLAVE_CLIENTE'] # para fines de modelación de momento el i del cliente no se requiere  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nuestro objetivo es impulsar la digitalización de la Banca de inversión, excluimos de nuestra muestra a las personas morales por las siguientes:\n",
    "\n",
    "    - Los clientes de Actinver en su mayoría son personas físicas\n",
    "    - Las personas morales pueden tener más de una persona que opere sus cuentas por lo que la elección o no de los servicios digitales puede estar comprometida o condicionada por reglas propias de su negocio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para excluir a las personas morales, la variable 'GENERO' nos permite identificarlas\n",
    "print(datos.GENERO.value_counts())\n",
    "index = datos[ datos['GENERO'] == 'SIN GENERO'].index\n",
    "datos.drop(index, inplace=True)\n",
    "datos = datos.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de los datos\n",
    "\n",
    "Veamos cómo se distribuyen nuestras variables con relación a variable a predecir, las cuales en su mayoría son categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colores UP\n",
    "colores =  np.array(  ['#A4061F', '#CFAB7A', '#00529B'  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(datos, aes(x='ENROLADO' , fill='ENROLADO')) + \\\n",
    "     geom_bar(aes(y = '(..count..)/sum(..count..)')) + \\\n",
    "     theme_minimal() + ylab('%') + scale_fill_manual(colores[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que la variable `CVE_UNIDAD_NEGOCIO` es la misma que `UNIDAD_NEGOCIO`, por lo que eliminamos la primera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(datos, aes(x='CVE_UNIDAD_NEGOCIO' , fill='UNIDAD_NEGOCIO')) + \\\n",
    "     geom_bar(aes(y = '(..count..)/sum(..count..)')) + \\\n",
    "     theme_minimal() + ylab('%') + scale_fill_manual(colores[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datos['CVE_UNIDAD_NEGOCIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = datos['EMISORA_FONDO'].value_counts().index.tolist()[::-1]\n",
    "print(len(temp))\n",
    "ggplot(datos) + aes(x='EMISORA_FONDO', y = '(..count..)/sum(..count..)',  fill='ENROLADO')  + geom_bar()  +\\\n",
    "  scale_x_discrete(limits=temp) + ylab('%') + theme_minimal() + \\\n",
    "     theme( legend_position='bottom', figure_size=(12, 16)) + \\\n",
    "     coord_flip() + scale_fill_manual(colores[1:3])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = datos['DESC_CE_FINANCIERO'].value_counts().index.tolist()[::-1]\n",
    "temp\n",
    "ggplot(datos) + aes(x='DESC_CE_FINANCIERO', y = '(..count..)/sum(..count..)',  fill='ENROLADO')  + geom_bar()  +\\\n",
    "  scale_x_discrete(limits=temp) + ylab('%') + theme_minimal() + \\\n",
    "     theme( legend_position='bottom', figure_size=(16, 12)) + \\\n",
    "     coord_flip() + scale_fill_manual(colores[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es común en las variables que reflejan ingresos, la variable`SALDO_CLIENTE` está sesgada positivamente por lo que trabajaremos en lo subsecuente con su logaritmo natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(datos, aes(x='SALDO_CLIENTE', fill='ENROLADO', color='ENROLADO')) + geom_density( alpha=0.1) + theme_minimal() +\\\n",
    "scale_fill_manual(colores[[0,2]]) + scale_color_manual(colores[[0,2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['SALDO_CLIENTE'] = np.log(datos['SALDO_CLIENTE'] + 1) # para precision numerica\n",
    "ggplot(datos, aes(x='SALDO_CLIENTE', fill='ENROLADO', color='ENROLADO')) + geom_density(alpha=0.1 ) + theme_minimal() +\\\n",
    "scale_fill_manual(colores[[0,2]]) + scale_color_manual(colores[[0,2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['TIPO_OPERACION'] =  datos['TIPO_OPERACION'].apply(lambda x : x.strip())\n",
    "temp = datos['TIPO_OPERACION'].value_counts().index.tolist()[::-1]\n",
    "ggplot(datos) + aes(x='TIPO_OPERACION', y = '(..count..)/sum(..count..)',  fill='ENROLADO')  + geom_bar()  +\\\n",
    "  scale_x_discrete(limits=temp) + ylab('%') + theme_minimal() + \\\n",
    "     theme(  figure_size=(16, 12)) + \\\n",
    "     coord_flip() + scale_fill_manual(colores[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(datos, aes(x='GENERO' , fill='ENROLADO')) + \\\n",
    "     geom_bar(aes(y = '(..count..)/sum(..count..)')) + \\\n",
    "     theme_minimal() + ylab('%') + scale_fill_manual(colores[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(datos, aes(x='Edad', fill='ENROLADO', color='ENROLADO')) + geom_density(alpha=0.1 ) + theme_minimal() +\\\n",
    "scale_fill_manual(colores[[0,2]]) + scale_color_manual(colores[[0,2]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Consideremos un escenario base para comparar nuestro modelo final de clasificación que refleje qué clientes tienen mayor probabilidad de utilizar los servicios de banca electrónica (aquí __sí hablamos de intervalos de probabilidad!__, consultar libro pág. 3 sección `1.1.2 Bayesian Inference in Practice`) considerando solo la variable el género del cliente.\n",
    "\n",
    "Partiendo de que sabemos que la distribución conjugada de una $Beta (\\alpha, \\beta)$ con una $X\\sim Binomial(N,p)$ (en nuestro caso definimos el éxito como que la persona use los servicios electrónicos con probabilidad $p$) es una distribución $Beta(\\alpha+X, \\beta+N-X)$ (consultar libro pág. 164), generamos el siguiente escenario: \n",
    "\n",
    "   - Un enfoque sin información, una distribución uniforme para la proporción de clientes con banca electrónica activa. Por lo que partimos de una distribución a priori $Beta(1,1)$, y obtengamos la probabilidad de que un cliente use los servicios electrónicos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirico = datos.groupby(['ENROLADO', 'GENERO']).size().reset_index(name='counts')\n",
    "empirico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' funciones para desplegar resultados'''\n",
    "#figsize(12,8)\n",
    "def _hist(data, label, color, **kwargs):\n",
    "    a = plt.hist(data, bins=80, density=True, histtype='stepfilled', alpha=.7, label=label, **kwargs, color=color)\n",
    "    return(a)\n",
    "''' funciones utiles'''\n",
    "def relative_increase(a,b):\n",
    "    return (a-b)/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from scipy.stats import beta\n",
    "np.random.seed(seed=0)\n",
    "# estimaciones muestrales\n",
    "digitales_fem = empirico.counts[0]\n",
    "digitales_mas = empirico.counts[1]\n",
    "fems = np.sum(empirico.counts[[0,2]]) \n",
    "mass = np.sum(empirico.counts[[1,3]]) \n",
    "\n",
    "# supuestos de la a priori uniforme\n",
    "alpha_aprior = beta_apriori = 1\n",
    "print(digitales_fem)\n",
    "print(digitales_mas)\n",
    "print(digitales_fem/fems  )\n",
    "print(digitales_mas/mass  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_fem = beta( alpha_aprior + digitales_fem, beta_apriori + fems - digitales_fem)\n",
    "posterior_mass = beta(alpha_aprior+ digitales_mas, beta_apriori + mass - digitales_mas)\n",
    "muestra_size = len(datos)*10\n",
    "print(muestra_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_post_fem = posterior_fem.rvs(muestra_size)\n",
    "sampling_post_mas = posterior_mass.rvs(muestra_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hist(sampling_post_fem, 'Mujeres', colores[0])\n",
    "_hist(sampling_post_mas, 'Hombres', colores[2])\n",
    "plt.xlabel('% Digitales')\n",
    "plt.ylabel('Densidad')\n",
    "plt.title('Distribución aposteriori de uso de servicios digitales por género')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde podemos notar fácilmente que los hombres tienen mayor probabilidad de hacer uso de los servicios digitales, ahora calculemos un intervalo con __probabilidad__ de 95% sobre el número de veces que la proporción de hombres utilice los servicios digitales en comparación del de mujeres.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mayor_p_mas = relative_increase( sampling_post_mas, sampling_post_fem)\n",
    "_hist(posterior_mayor_p_mas, '', color='#5C4985')\n",
    "plt.xlabel('% ')\n",
    "plt.ylabel('Densidad')\n",
    "plt.title('Distribución aposteriori de incremento de uso de servicios digitales\\n de los clientes hombres respecto a los clientes mujeres')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intervalo de 95% de probabilidad de mayor uso de servicios digitales en hombres con respecto al de mujeres')\n",
    "print( '[' + str( np.round(np.percentile(posterior_mayor_p_mas, q=.025) , 4)) + ','+\\\n",
    "      str( np.round(np.percentile(posterior_mayor_p_mas, q=.975) , 4)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de los modelos\n",
    "\n",
    "En vista de que la mayoría de las columnas con las que trabajaremos son nominales vamos a realizar un cambio de encoding. Como tenemos algunas variables con más de 100 categorías en lugar de hacer dummies estas variables usaremos un encoding que mapea a cada categoría con un entero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variables = datos.select_dtypes(['object']).columns\n",
    "datos[cat_variables] = datos[cat_variables].apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizamos las variables para tener mayor estabilidad numerica\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "variables = ['UNIDAD_NEGOCIO', 'EMISORA_FONDO', 'DESC_CE_FINANCIERO', 'SALDO_CLIENTE', 'TIPO_OPERACION', 'GENERO', 'Edad']\n",
    "scaler.fit(datos[variables])\n",
    "datos[variables] = scaler.transform(datos[variables])\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as logistic_model:\n",
    "    pm.glm.GLM.from_formula('ENROLADO ~ UNIDAD_NEGOCIO + EMISORA_FONDO + DESC_CE_FINANCIERO +\\\n",
    "                            SALDO_CLIENTE + TIPO_OPERACION +GENERO',\n",
    "                            datos,\n",
    "                            family=pm.glm.families.Binomial())\n",
    "    trace = pm.sample(1000, tune=1000, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(mp.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en conjunto de entrenamiento y de prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split( datos, y, test_size = 0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de los modelos\n",
    "\n",
    "En esta sección creamos los 4 modelos correspondientes a los clasificadores: kmeans, mezcla de gaussianas, bosques aleatorios o random forest y el XGBoosting (Gradient boosting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos los modelos que evaluaremos\n",
    "modelos = []\n",
    "from sklearn.cluster import KMeans\n",
    "modelos.append( KMeans(n_clusters = 2).fit(train_x) ) # 2 centros pues son el numero de categorias a clasificar\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "modelos.append( GaussianMixture (n_components = 2, covariance_type = 'full').fit(train_x))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelos.append( RandomForestClassifier(random_state=0).fit(train_x, train_y) )\n",
    "\n",
    "import xgboost as xgb\n",
    "modelos.append( xgb.XGBClassifier(random_state=1,learning_rate=0.01).fit(train_x, train_y) )\n",
    "# tal vez sea necesario instalar la libraria y reiniciar el notebook \n",
    "# !pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer apriori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de los modelos\n",
    "\n",
    "Entrenamos y evaluamos por medio de la precisión (*accuracy*) en el conjunto de prueba el desempeño de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "precision = []\n",
    "for i in modelos:\n",
    "    y_temp = i.predict(test_x)\n",
    "    precision.append( accuracy_score(test_y, y_temp) )\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De donde obtenemos que el random forest es el segundo modelo con mayor precisión, solo marginalmente en comparación al XGB, sin embargo como RF es más interpretable lo escogemos como ganador. Procedemos a afinar el modelo por medio de sus parámetros con una búsqueda de grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "for _ in range(0, 20):\n",
    "    model = RandomForestClassifier(n_estimators = _*4+2, random_state=0 )\n",
    "    model.fit(train_x, train_y)\n",
    "    y_temp = model.predict(test_x)    \n",
    "    grid[str(_*10+1)] =  accuracy_score(test_y, y_temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(final)\n",
    "final = scaler.transform(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde el mejor modelo lo obtenemos con 101 árboles en el bosque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 101, random_state=0 )\n",
    "model.fit(final, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir los limites de probabilidad con los que clasificaremos, obtenemos los dos centroides de cada estrato formado por los valores ‘BANCO’ y ‘CASA DE BOLSA’ de la variable ‘UNIDAD_NEGOCIO’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "kmeans = KMeans(n_clusters = 3, max_iter = 2000, random_state=0).fit( final[index_banco, :] )\n",
    "banco_limites  = model.predict_proba( kmeans.cluster_centers_ )[:, 1] # probabilidad de tener banca electronica \n",
    "print(banco_limites)\n",
    "kmeans = KMeans(n_clusters = 3, max_iter = 2000, random_state=0).fit( final[index_casa_de_bolsa, :] )\n",
    "casa_limites  = model.predict_proba( kmeans.cluster_centers_ )[:,1] # probabilidad de tener banca electronica \n",
    "print(casa_limites)\n",
    "\n",
    "y_prob = pd.DataFrame(model.predict_proba(final)[:,  1]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limite_banco = np.max(banco_limites)\n",
    "limite_casa = np.max(casa_limites)\n",
    "print(limite_banco)\n",
    "print(limite_casa)\n",
    "y_prob.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final =pd.DataFrame(final)\n",
    "final['STATUS_BE'] = y\n",
    "y_prob = y_prob.reset_index(drop = True)\n",
    "final[['Prediction']] = y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASIFICACAMOS CON LOS LIMITES QUE CALCULAMOS\n",
    "mask = final['Prediction'][ index_banco ] > limite_banco\n",
    "final['Prediction'][ index_banco ] = np.where( mask == True, 1, 0 )\n",
    "mask = final['Prediction'][ index_casa_de_bolsa ] > limite_casa\n",
    "final['Prediction'][ index_casa_de_bolsa ] = np.where( mask == True, 1, 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix( y, final['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(222)/(2090+222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así con el modelo (Random Forest) elegido y los limites de probabilidad para clasificar, calculados con base en la clasificación no supervisada de los estratos de ‘BANCO’ y ‘CASA DE BOLSA’ , podemos identificar a 222 usuarios más propensos a no requerir los servicios del call center. \n",
    "Esto representa cerca del 10% en disminución de la carga de trabajo y costos que conlleva operar el CC. \n",
    "\n",
    "Es de notar que el __modelo seleccionado no clasifica a ningún usuario que no usa el CC como posible usuario del mismo__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuciones a posteriori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
